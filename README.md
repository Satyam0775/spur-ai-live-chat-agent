# Spur AI Live Chat â€“ Take Home Assignment

This project is a mini AI-powered customer support chat application built as part of the **Spur â€“ Founding Full-Stack Engineer Take-Home Assignment**.

It simulates a live chat widget where users can ask questions and receive contextual responses from an AI support agent using a real LLM API.

**Deployed Project Link:**  
[[https://spur-ai-live-chat-6s9a.onrender.com/](https://spur-ai-live-chat-ntxl.onrender.com/)](https://spur-ai-live-chat-ntxl.onrender.com)


---

## ğŸš€ Features

- AI-powered live chat using a real LLM (Groq LLaMA)
- Session-based conversation memory
- Backend API built with FastAPI
- Frontend served directly from the backend
- SQLite database for conversation persistence
- Clean, modular, production-style architecture

---

## ğŸ§± Tech Stack

### Backend
- Python
- FastAPI
- SQLAlchemy
- SQLite

### Frontend
- HTML
- CSS
- Vanilla JavaScript

### LLM
- Groq API (LLaMA 3.1)

---

## ğŸ“ Project Structure
# Spur AI Live Chat â€“ Take Home Assignment

This project is a mini AI-powered customer support chat application built as part of the **Spur â€“ Founding Full-Stack Engineer Take-Home Assignment**.

It simulates a live chat widget where users can ask questions and receive contextual responses from an AI support agent using a real LLM API.

---

## ğŸš€ Features

- AI-powered live chat using a real LLM (Groq LLaMA)
- Session-based conversation memory
- Backend API built with FastAPI
- Frontend served directly from the backend
- SQLite database for conversation persistence
- Clean, modular, production-style architecture

---

## ğŸ§± Tech Stack

### Backend
- Python
- FastAPI
- SQLAlchemy
- SQLite

### Frontend
- HTML
- CSS
- Vanilla JavaScript

### LLM
- Groq API (LLaMA 3.1)

---

## ğŸ“ Project Structure
# Spur AI Live Chat â€“ Take Home Assignment

This project is a mini AI-powered customer support chat application built as part of the **Spur â€“ Founding Full-Stack Engineer Take-Home Assignment**.

It simulates a live chat widget where users can ask questions and receive contextual responses from an AI support agent using a real LLM API.

---

## ğŸš€ Features

- AI-powered live chat using a real LLM (Groq LLaMA)
- Session-based conversation memory
- Backend API built with FastAPI
- Frontend served directly from the backend
- SQLite database for conversation persistence
- Clean, modular, production-style architecture

---

## ğŸ§± Tech Stack

### Backend
- Python
- FastAPI
- SQLAlchemy
- SQLite

### Frontend
- HTML
- CSS
- Vanilla JavaScript

### LLM
- Groq API (LLaMA 3.1)

---

## ğŸ“ Project Structure
backend/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ api/ # API routes
â”‚ â”œâ”€â”€ core/ # Config & system prompt
â”‚ â”œâ”€â”€ db/ # DB models, session, CRUD
â”‚ â”œâ”€â”€ services/ # LLM integration
â”‚ â”œâ”€â”€ utils/ # Input validation
â”‚ â””â”€â”€ static/ # Frontend (HTML/CSS/JS)
â”‚
â”œâ”€â”€ main.py # FastAPI entry point
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env


---

## âš™ï¸ How to Run Locally

### 1ï¸âƒ£ Clone the repository
```bash
git clone <your-repo-url>
cd backend

2ï¸âƒ£ Create & activate virtual environment
python -m venv venv
venv\Scripts\activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Setup environment variables

Create a .env file in the backend folder:

GROQ_API_KEY=your_groq_api_key_here

5ï¸âƒ£ Run the application
python -m uvicorn main:app --reload

ğŸŒ Access the App

Frontend UI:
http://127.0.0.1:8000/

API Docs (Swagger):
http://127.0.0.1:8000/docs

ğŸ¤– LLM Details

Provider: Groq

Model: LLaMA 3.1

Prompting Strategy

A system prompt defines store policies (shipping, returns, support hours)

Conversation history is included to maintain context

Guardrails

Graceful error handling

Max token limit for cost control

ğŸ—ƒï¸ Data Persistence

SQLite database (chat.db)

Tables:

conversations

messages

Each session maintains full chat history

ğŸ§ª Backend API Testing (Postman)

The backend API was tested independently using Postman to verify correctness without relying on the frontend.

Endpoint
POST /chat/message

URL
http://127.0.0.1:8000/chat/message

Headers
Content-Type: application/json

Request Body (JSON)
{
  "message": "What is your return policy?"
}

Sample Response
{
  "reply": "Our return policy is a 7-day no-questions-asked return policy. If you're not satisfied with your purchase, you can return it within 7 days for a full refund.",
  "sessionId": "ea4cb8d5-03b0-4994-9b4c-a991c46efa33"
}

Session Continuity Test
{
  "message": "Do you ship to the USA?",
  "sessionId": "ea4cb8d5-03b0-4994-9b4c-a991c46efa33"
}


The AI successfully maintained context across messages, confirming correct session-based conversation handling.

âœ… Verification Summary

Backend API works independently via Postman
LLM responses are generated correctly
Conversation history is persisted per session
Frontend and backend behavior are consistent
This confirms the backend is robust and correctly implemented as per the assignment requirements.

ğŸ§  Design Decisions
Frontend served via FastAPI static files to avoid CORS issues
Clean separation of concerns (API, DB, services, utils)
Simple UI to focus on functionality over styling
No authentication to keep scope aligned with the assignment

ğŸ”„ Trade-offs & Future Improvements

If I had more time, I would:
Add streaming responses for better UX
Improve frontend UI/UX
Add Redis for session caching

Add authentication and multi-agent support
Deploy with CI/CD

Statusâœ… 
All core requirements of the Spur Founding Full-Stack Engineer take-home assignment are fully implemented and working end-to-end.
